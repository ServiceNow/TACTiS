{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from loss.dilate_loss import DilateLoss \n",
    "import torch\n",
    "from pts import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA:\n",
      "  NAMES_OF_BAD_FILES: ['23EF07CA2AD4829E5A3DC550DDF212BE4DF432357587117B8790BF98BDB04EF52277040A3F9FB30FFFB875701FFFF0FD_202102230745.xlsx', '490175DD47C1E53DE74F180C750AA30055974E0FEFDF45C5B52C8ADC028C5B7EEC0C441B7C1869130D1B194AD47E4EDD_202103181245.xlsx']\n",
      "  OVERLAP_LEN: 10\n",
      "  PATH: ../data/medical_dataset/2nd_version_data\n",
      "  PREDICT_MAGNITUDE: False\n",
      "  PROPOFOL_ID: 5\n",
      "  SOURCE_LEN: 20\n",
      "  TARGET_LEN: 20\n",
      "  TEST_PERCENT: 0.2\n",
      "MODEL:\n",
      "  ACTIVATION: gelu\n",
      "  DEC_IN: 11\n",
      "  DIM_OUT: 1\n",
      "  DROPOUT: 0.05\n",
      "  D_FF: 2048\n",
      "  D_LAYERS: 1\n",
      "  D_MODEL: 512\n",
      "  ENC_IN: 11\n",
      "  E_LAYERS: 2\n",
      "  FACTOR: 1\n",
      "  MOV_AVG: 11\n",
      "  N_HEADS: 8\n",
      "  POS_ENC_N: 2\n",
      "SYSTEM:\n",
      "  SEED: 2022\n",
      "TEST:\n",
      "  PREDICTIONS_TO_FILES: True\n",
      "  SAVE_PATH: results\n",
      "TRAIN:\n",
      "  BATCH: 128\n",
      "  CRITERION: MSE\n",
      "  DILATE_GAMMA: 0.0001\n",
      "  EPOCHS: 10\n",
      "  LR: 0.0001\n",
      "  USE_ALL_DATA: True\n",
      "Total number of files read: 628\n",
      "Number of correct dataframes: 605\n",
      "TRAIN FILES: 484\n",
      "TEST FILES: 121\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os, sys; sys.path.append('..')\n",
    "from share_src.config import read_config\n",
    "from share_src.data import make_datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cfg = read_config()\n",
    "train_dataset, test_dataset = make_datasets(cfg)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(train_dataset, \n",
    "    batch_size=cfg.TRAIN.BATCH, shuffle=True, drop_last=True)\n",
    "testloader = DataLoader(test_dataset, \n",
    "    batch_size=cfg.TRAIN.BATCH, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tactis.model.tactis import TACTiS\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def create_net():\n",
    "    net = TACTiS(\n",
    "        num_series=11,\n",
    "        series_embedding_dim=5,\n",
    "        input_encoder_layers=3,\n",
    "        input_encoding_normalization=True,\n",
    "        data_normalization=\"standardization\",\n",
    "        loss_normalization=\"series\",\n",
    "        positional_encoding={\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "        temporal_encoder={\n",
    "            \"attention_layers\": 3,\n",
    "            \"attention_heads\": 3,\n",
    "            \"attention_dim\": 16,\n",
    "            \"attention_feedforward_dim\": 16,\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "        copula_decoder={\n",
    "            \"min_u\": 0.01,\n",
    "            \"max_u\": 0.99,\n",
    "            \"attentional_copula\": {\n",
    "                \"attention_heads\": 3,\n",
    "                \"attention_layers\": 3,\n",
    "                \"attention_dim\": 16,\n",
    "                \"mlp_layers\": 3,\n",
    "                \"mlp_dim\": 16,\n",
    "                \"resolution\": 50,\n",
    "            },\n",
    "            \"dsf_marginal\": {\n",
    "                \"mlp_layers\": 2,\n",
    "                \"mlp_dim\": 8,\n",
    "                \"flow_layers\": 2,\n",
    "                \"flow_hid_dim\": 8,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TACTiS(\n",
       "  (series_encoder): Embedding(11, 5)\n",
       "  (encoder): TemporalEncoder(\n",
       "    (layer_timesteps): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_series): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): CopulaDecoder(\n",
       "    (copula): AttentionalCopula(\n",
       "      (dimension_shifting_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "      (key_creators): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (value_creators): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=49, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (attention_dropouts): ModuleList(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_layer_norms): ModuleList(\n",
       "        (0): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_forwards): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward_layer_norms): ModuleList(\n",
       "        (0): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dist_extractors): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=16, out_features=50, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (marginal): DSFMarginal(\n",
       "      (marginal_flow): DeepSigmoidFlow(\n",
       "        (layers): Sequential(\n",
       "          (0): SigmoidFlow()\n",
       "          (1): SigmoidFlow()\n",
       "        )\n",
       "      )\n",
       "      (marginal_conditioner): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=8, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=8, out_features=48, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (input_encoder): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=48, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=48, out_features=48, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=48, out_features=48, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736681bcdd614ba68e79252458558e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eb30e7e1cc4e58a35f03eeb82a16f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_275243/2670861062.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_275243/2670861062.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c2094afa5b4517ad22e223c2ad6c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a53ef0ca95492d97b18e12eadeebe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_input = cfg.DATA.SOURCE_LEN\n",
    "N_output = cfg.DATA.TARGET_LEN  \n",
    "sigma = 0.01\n",
    "alpha=0.5\n",
    "gamma = cfg.TRAIN.DILATE_GAMMA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train_model(net, loss_type, learning_rate, epochs=1000, gamma = 0.001,\n",
    "                print_every=50,eval_every=50, verbose=1, Lambda=1, alpha=0.5):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "    criterion_mse = torch.nn.MSELoss()\n",
    "    criterion_dilate = DilateLoss(alpha, gamma, device)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)): \n",
    "        pbar = tqdm(enumerate(trainloader, 0), \n",
    "            total=len(trainloader), leave=False)\n",
    "        for i, data in pbar:\n",
    "            inputs, target, _ = data\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "            target = torch.tensor(target, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Remove positional data\n",
    "            pos_inputs = inputs[:,:,-1] + inputs[:,:,-2]\n",
    "            pos_target = (target[:,cfg.DATA.OVERLAP_LEN:,-1] + \n",
    "                          target[:,cfg.DATA.OVERLAP_LEN:,-1])\n",
    "            inputs = inputs[:,:,:-2]\n",
    "            target = target[:,cfg.DATA.OVERLAP_LEN:,:-2]\n",
    "            \n",
    "            # reshape data to fit model\n",
    "            batch_size, seq_len, feature_dim = inputs.shape\n",
    "            inputs = inputs.reshape(batch_size, feature_dim, seq_len)\n",
    "            target = target.reshape(batch_size, feature_dim, seq_len)\n",
    "            # pos_inputs = pos_inputs[:,:,1]\n",
    "            # pos_target = pos_target[:,:,0]\n",
    "\n",
    "            '''\n",
    "            inputs/hist_value [batch_size, series, hist_length]\n",
    "            target/pred_value [batch_size, series, pred_length]\n",
    "            pos_inputs/hist_time  [batch_size, hist_length]\n",
    "            pos_target/pred_time  [batch_size, pred_length]\n",
    "            '''\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = net.loss(pos_inputs, inputs, pos_target, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses += [loss.item()]\n",
    "            pbar.set_description('loss: {:.4f}'.format(loss.item()))\n",
    "    \n",
    "    model = net\n",
    "    return model, losses\n",
    "\n",
    "trained_model, losses = train_model(\n",
    "    model,\n",
    "    loss_type='dilate',\n",
    "    learning_rate=cfg.TRAIN.LR,\n",
    "    epochs=cfg.TRAIN.EPOCHS, \n",
    "    gamma=gamma, \n",
    "    print_every=50, \n",
    "    eval_every=50,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(net, optimizer, batch_size, data, hist_length, pred_length):\n",
    "    max_idx = data.shape[1] - (hist_length + pred_length)\n",
    "    \n",
    "    hist_values = []\n",
    "    pred_values = []\n",
    "    for _ in range(batch_size):\n",
    "        idx = np.random.randint(0, max_idx)\n",
    "        hist_values.append(data[:, idx:idx+hist_length])\n",
    "        pred_values.append(data[:, idx+hist_length:idx+hist_length+pred_length])\n",
    "    \n",
    "    # [batch, series, time steps]\n",
    "    hist_value = torch.Tensor(hist_values).to(device)\n",
    "    pred_value = torch.Tensor(pred_values).to(device)\n",
    "    hist_time = torch.arange(0, hist_length, device=device)[None, :].expand(batch_size, -1)\n",
    "    pred_time = torch.arange(hist_length, hist_length + pred_length, device=device)[None, :].expand(batch_size, -1)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = net.loss(hist_time, hist_value, pred_time, pred_value)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_net()\n",
    "data_train, data_test = generate_random_data()\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "NUM_EPOCHS = 1000  # The model is very slow to train\n",
    "NUM_BATCHES = 100\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_sum = 0\n",
    "    for batch in range(NUM_BATCHES):\n",
    "        running_sum += step(net, optimizer, 256, data_train, 10, 10)\n",
    "    avg_loss.append(running_sum / NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, num_samples, data, hist_length, pred_length):\n",
    "    max_idx = data.shape[1] - (hist_length + pred_length)\n",
    "    \n",
    "    idx = np.random.randint(0, max_idx)\n",
    "    hist_value = torch.Tensor(data[:, idx:idx+hist_length]).to(device)\n",
    "    pred_value = torch.Tensor(data[:, idx+hist_length:idx+hist_length+pred_length]).to(device)\n",
    "    \n",
    "    # [batch, series, time steps]\n",
    "    hist_value = hist_value[None, :, :]\n",
    "    pred_value = pred_value[None, :, :]\n",
    "    hist_time = torch.arange(0, hist_length, device=device)[None, :]\n",
    "    pred_time = torch.arange(hist_length, hist_length + pred_length, device=device)[None, :]\n",
    "\n",
    "    samples = net.sample(num_samples, hist_time, hist_value, pred_time)\n",
    "    \n",
    "    return samples, torch.cat([hist_value, pred_value], axis=2), torch.cat([hist_time, pred_time], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, pred_value, timesteps = sample(net, 1000, data_test, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_series(samples, target, timesteps, index):\n",
    "    s_samples = samples[0, index, :, :].cpu().numpy()\n",
    "    s_timesteps = timesteps[0, :].cpu().numpy()\n",
    "    s_target = target[0, index, :].cpu().numpy()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    for zorder, quant, color, label in [\n",
    "        [1, 0.05, (0.75,0.75,1), \"5%-95%\"],\n",
    "        [2, 0.10, (0.25,0.25,1), \"10%-90%\"],\n",
    "        [3, 0.25, (0,0,0.75), \"25%-75%\"],\n",
    "    ]:\n",
    "        plt.fill_between(\n",
    "            s_timesteps,\n",
    "            np.quantile(s_samples, quant, axis=1),\n",
    "            np.quantile(s_samples, 1 - quant, axis=1),\n",
    "            facecolor=color,\n",
    "            interpolate=True,\n",
    "            label=label,\n",
    "            zorder=zorder,\n",
    "        )\n",
    "    \n",
    "    plt.plot(\n",
    "        s_timesteps,\n",
    "        np.quantile(s_samples, 0.5, axis=1),\n",
    "        color=(0.5,0.5,0.5),\n",
    "        linewidth=3,\n",
    "        label=\"50%\",\n",
    "        zorder=4,\n",
    "    )\n",
    "    \n",
    "    plt.plot(s_timesteps, s_target, color=(0, 0, 0), linewidth=2, zorder=5, label=\"ground truth\")\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [1, 2, 3, 4, 0]\n",
    "    plt.legend([handles[idx] for idx in order], [labels[idx] for idx in order])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c627c90d444f42ac43cabb74db3a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a725f2c94c364eb6a7c8a37e0bff0853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140928/4153988318.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_140928/4153988318.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ivan/Projects/tactis/my_data_processor.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model, losses\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# if(verbose):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m#     if (epoch % print_every == 0):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m#         print('epoch ', epoch, ' loss ',loss.item(),' loss shape ',loss_shape.item(),' loss temporal ',loss_temporal.item())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# trainloader = DataLoader(train_dataset, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m#     batch_size=cfg.TRAIN.BATCH, shuffle=True, drop_last=True)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m trained_model, losses \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     loss_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdilate\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mTRAIN\u001b[39m.\u001b[39;49mLR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mTRAIN\u001b[39m.\u001b[39;49mEPOCHS, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     gamma\u001b[39m=\u001b[39;49mgamma, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     print_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     eval_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/ivan/Projects/tactis/my_data_processor.ipynb Cell 4\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, loss_type, learning_rate, epochs, gamma, print_every, eval_every, verbose, Lambda, alpha)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m batch_size, N_output \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]                     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# loss_mse,loss_shape,loss_temporal = torch.tensor(0),torch.tensor(0),torch.tensor(0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# if (loss_type=='mse'):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#     loss_mse = criterion(target,outputs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m#     loss = loss_mse                   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (loss_type\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdilate\u001b[39m\u001b[39m'\u001b[39m):    \n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/torch/nn/modules/module.py:201\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import sys\n",
    "REPO_NAME = \"tactis\"\n",
    "def get_repo_basepath():\n",
    "    cd = os.path.abspath(os.curdir)\n",
    "    return cd[:cd.index(REPO_NAME) + len(REPO_NAME)]\n",
    "REPO_BASE_PATH = get_repo_basepath()\n",
    "sys.path.append(REPO_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pts import Trainer\n",
    "import torch\n",
    "from tactis.gluon.estimator import TACTiSEstimator\n",
    "from tactis.gluon.dataset import generate_backtesting_datasets\n",
    "from tactis.gluon.metrics import compute_validation_metrics\n",
    "from tactis.gluon.plots import plot_four_forecasts\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e874ffd1a384484a32390255179c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ivan/Projects/tactis/my_data_processor.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ivan/Projects/tactis/my_data_processor.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictor \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mtrain(trainloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/pts/model/estimator.py:179\u001b[0m, in \u001b[0;36mPyTorchEstimator.train\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    178\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m    180\u001b[0m         training_data,\n\u001b[1;32m    181\u001b[0m         validation_data,\n\u001b[1;32m    182\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    183\u001b[0m         prefetch_factor\u001b[39m=\u001b[39;49mprefetch_factor,\n\u001b[1;32m    184\u001b[0m         shuffle_buffer_length\u001b[39m=\u001b[39;49mshuffle_buffer_length,\n\u001b[1;32m    185\u001b[0m         cache_data\u001b[39m=\u001b[39;49mcache_data,\n\u001b[1;32m    186\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    187\u001b[0m     )\u001b[39m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/pts/model/estimator.py:151\u001b[0m, in \u001b[0;36mPyTorchEstimator.train_model\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     validation_iter_dataset \u001b[39m=\u001b[39m TransformedIterableDataset(\n\u001b[1;32m    134\u001b[0m         dataset\u001b[39m=\u001b[39mvalidation_data,\n\u001b[1;32m    135\u001b[0m         transform\u001b[39m=\u001b[39mtransformation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         cache_data\u001b[39m=\u001b[39mcache_data,\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m     validation_data_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    142\u001b[0m         validation_iter_dataset,\n\u001b[1;32m    143\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer(\n\u001b[1;32m    152\u001b[0m     net\u001b[39m=\u001b[39;49mtrained_net,\n\u001b[1;32m    153\u001b[0m     train_iter\u001b[39m=\u001b[39;49mtraining_data_loader,\n\u001b[1;32m    154\u001b[0m     validation_iter\u001b[39m=\u001b[39;49mvalidation_data_loader,\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m TrainOutput(\n\u001b[1;32m    158\u001b[0m     transformation\u001b[39m=\u001b[39mtransformation,\n\u001b[1;32m    159\u001b[0m     trained_net\u001b[39m=\u001b[39mtrained_net,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     ),\n\u001b[1;32m    163\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/pts/trainer.py:63\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[0;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(train_iter, total\u001b[39m=\u001b[39mtotal) \u001b[39mas\u001b[39;00m it:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mfor\u001b[39;00m batch_no, data_entry \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(it, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     64\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     66\u001b[0m         inputs \u001b[39m=\u001b[39m [v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m data_entry\u001b[39m.\u001b[39mvalues()]\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:103\u001b[0m, in \u001b[0;36mTransformedDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[DataEntry]:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformation(\n\u001b[1;32m    104\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_dataset, is_train\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_train\n\u001b[1;32m    105\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:124\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[0;34m(self, data_it, is_train)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m    123\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator:\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_it:\n\u001b[1;32m    125\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_transform(data_entry\u001b[39m.\u001b[39mcopy(), is_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:124\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[0;34m(self, data_it, is_train)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m    123\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator:\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_it:\n\u001b[1;32m    125\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_transform(data_entry\u001b[39m.\u001b[39mcopy(), is_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:178\u001b[0m, in \u001b[0;36mFlatMapTransformation.__call__\u001b[0;34m(self, data_it, is_train)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m    176\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator:\n\u001b[1;32m    177\u001b[0m     num_idle_transforms \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_it:\n\u001b[1;32m    179\u001b[0m         num_idle_transforms \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatmap_transform(data_entry\u001b[39m.\u001b[39mcopy(), is_train):\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:128\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[0;34m(self, data_it, is_train)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_transform(data_entry\u001b[39m.\u001b[39mcopy(), is_train)\n\u001b[1;32m    127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:126\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[0;34m(self, data_it, is_train)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_it:\n\u001b[1;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_transform(data_entry\u001b[39m.\u001b[39;49mcopy(), is_train)\n\u001b[1;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/_base.py:141\u001b[0m, in \u001b[0;36mSimpleTransformation.map_transform\u001b[0;34m(self, data, is_train)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_transform\u001b[39m(\u001b[39mself\u001b[39m, data: DataEntry, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataEntry:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-dev/lib/python3.9/site-packages/gluonts/transform/feature.py:250\u001b[0m, in \u001b[0;36mAddObservedValuesIndicator.transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, data: DataEntry) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataEntry:\n\u001b[0;32m--> 250\u001b[0m     value \u001b[39m=\u001b[39m data[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_field]\n\u001b[1;32m    251\u001b[0m     nan_entries \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(value)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimputation_method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.batch_size = 16\n",
    "metrics = compute_validation_metrics(\n",
    "    predictor=predictor,\n",
    "    dataset=test_data,\n",
    "    window_length=estimator.history_length + estimator.prediction_length,\n",
    "    num_samples=100,\n",
    "    split=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_data, predictor=predictor, num_samples=100\n",
    ")\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_four_forecasts(\n",
    "    forecasts=forecasts,\n",
    "    targets=targets,\n",
    "    selection=[(0, 0), (1, 5), (2, 10), (3, 15)],\n",
    "    tick_freq=\"day\",\n",
    "    history_length=estimator.history_length,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rl-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4cd5286c77fa74a709fcc6c5da8d4519fc350b4e9f17441beca3c06a8c397ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
